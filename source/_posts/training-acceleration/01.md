---
title: ZeRO论文简读
categories: [笔记]
tags: [训练加速, 系统, AI, 论文]
date: 2025-08-08
---

之前看了李沐老师的ZeRO和参数服务器的讲解讲解，没有太看懂。

重新自己学一下。

另外今天gpt5出了，好好问问他。

<!--more-->

ZeRO的出发点是：**百亿、万亿参数大模型的训练瓶颈不是算力，而是显存。**

即使多卡训练，单卡的显存限制会导致模型根本放不下。



# 1 前置知识

## 1.1 数据并行（Data Parallel, DP）

> **每张卡**：
>
> - 一份**完整的模型参数**（weights）
> - 一份**完整的优化器状态**（例如 Adam 的动量、方差）
> - 一份**完整的梯度**（在反向传播时生成）
>
> **怎么并行**：
>
> 1. 不同 GPU 各自拿不同的 mini-batch 数据，独立做前向和反向。
> 2. 反向结束后，通过 **all-reduce** 把各自的梯度求平均（同步）。
> 3. 每张卡用平均后的梯度更新自己那份完整参数。
>
> **特点**：
>
> - 算法简单，计算效率高。
> - **缺点**：显存占用大，冗余严重——每张卡都放全套参数+状态。

简单说：每张卡并行计算不同的batch，每一轮结束后求平均（同步）

这很显然会有一些同步开销。有几个常用的做法：

1. 通信与计算重叠

    反向传播中，算出一层的梯度后就开始通信。把通信和计算并行。

2. 参数服务器

    参数存在服务器里，谁先完成谁更新参数。但是会引入“梯度延迟”，获取到的参数并不是最新的，影响收敛质量。

3. 负载均衡、流水优化等：减小短板效应，让各个卡的用时尽量一致。



## 1.2 模型并行（Model Parallel, MP）

> **每张卡**：
>
> - 只放**部分参数**（例如某一层的一部分权重，或某个矩阵的分块）
> - 对应部分的中间激活（activations）
>
> **怎么并行**：
>
> 1. 前向传播时，不同 GPU 负责不同的计算块，计算结果需要互相通信。
> 2. 反向传播时，同理，需要通信传递梯度。
>
> **特点**：
>
> - 显存压力小（因为每张卡只存部分模型）。
> - **缺点**：计算粒度小、通信频繁，跨节点时性能下降明显。

每个卡存模型的一部分。

前向传播得从模型的第一个部分开始计算，将计算结果发送给下一张卡。

反向传播从最后一个部分开始计算梯度，将结果传回上一张卡。

模型并行并没有加速，频繁的通信显然降低了计算效率。



# 2 ZeRO

根据前置知识，现有方案要么增加显存浪费来提升计算效率，要么降低计算效率减少显存浪费。

所以ZeRO的目的就是：**在不降低计算/通信效率的前提下**，极大减少显存占用，让模型规模可以按设备数量线性扩展。



## 2.1 显存占用的构成

