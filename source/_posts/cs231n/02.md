---
title: cs231n - 反向传播
categories: [笔记]
tags: [AI, cs231n, 数学]
date: 2025-06-03
---

反向传播，课程看完了就是链式法则求导，没做笔记，做作业的时候记一下。

核心还是看最后的数学补充。

<!--more-->

# 全连接网络的反向传播

$$
out = xW + b
$$

已知 $dout = \frac{\partial L}{\partial out}$ ，反向传播要求 $dx$, $dW$, $db$ ，shape应该分别和 $x$ $W$ $b$ 相同。
$$
dx = \frac{\partial L}{\partial x} = \frac{\partial L}{\partial out} \frac{\partial out}{\partial x} = dout \cdot W^T 
$$

> 下面的说法可能**不够正确**、**不够简洁**，仅供参考

矩阵求导的结果就是对应的雅可比矩阵。经常很难想清楚到底是左乘、右乘，要不要求转置等等。

一个比较好的方法是把所有变量的 shape 写出来。

x=dx: (n, m)
W=dW: (m, d)
out=dout=xW: (n, d)
b=db: (1, d)

根据链式法则，dx 一定是 dout 和 W 的乘积。看shape就直接写出来：

(n, m) = (n, d) · (d, m) => $dx = dout · W^T$

同理可以写出：

(m, d) = (m, n) · (n, d) => $dW = x^T · dout$

对于 db，在参与计算的时候自动进行了广播，变成了 (n, d) ，可以有：

(n, d) = (n, d) => $db' = dout$ 

最后计算 db 的时候要把每行的 n 个元素加起来：

```py
db = np.sum(dout, axis=0, keepdims=True)
```



# ReLU的反向传播

公式：
$$
\text{ReLU}(x) = max(0, x) = 
\begin{cases}
x & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}
$$
分段函数分段求导：
$$
\frac{d}{dx} \text{ReLU}(x) =
\begin{cases}
1 & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}
$$




# 数学补充

## 1 雅可比矩阵（Jacobian）

雅可比矩阵是一个函数的一节偏导数矩阵。

设：
$$
\mathbf{y} = f(\mathbf{x}) \in \mathbb{R}^m, \quad \mathbf{x} \in \mathbb{R}^n
$$

> 这里的 x 和 y 都是向量，即(1, n) 或 (n,1)的形状

那么 **雅可比矩阵** 是：
$$
J = \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n} \end{bmatrix} \in \mathbb{R}^{m \times n}
$$
每行是一个输出变量 $y_i$ 对所有输入变量 $x_j$ 的偏导，表示的是 $\nabla y_i$。

其实很好理解，$y_1 = w_1 x_1+w_2x_2+\cdots+x_n$ ，矩阵的第一行就是 y1 相对于 x1, x2, ..., xn 的倒数。



以一个具体例子为例：
$$
z = xW + b 
$$
 $\frac{\partial z}{\partial x}$ 就是 z 对 x 的雅可比矩阵。

我们先设定具体形状：

- $x \in \mathbb{R}^{1 \times D}$：输入是**一个样本的特征行向量**

- $W \in \mathbb{R}^{D \times M}$：权重矩阵（D 输入维度，M 输出维度）

- $b \in \mathbb{R}^{1 \times M}$：偏置（广播加法）

- 那么：

    $z = xW + b \in \mathbb{R}^{1 \times M}$



写成分量形式：
$$
z_j= \sum_{i=1}^D x_i W_{i,j} + b_j, j=1,\dots,M
$$
所以：
$$
\frac{\partial z_j} {\partial x_i} = W_{i,j}
$$


直观的看，W的展开为：
$$
W = \begin{bmatrix} W_{1,1} & W_{1,2} & \cdots & W_{1,M} \\ W_{2,1} & W_{2,2} & \cdots & W_{2,M} \\ \vdots  & \vdots  & \ddots & \vdots \\ W_{D,1} & W_{D,2} & \cdots & W_{D,M} \end{bmatrix} \quad \in \mathbb{R}^{D \times M}
$$
而雅可比矩阵是：
$$
\frac{\partial z}{\partial x} =
\begin{bmatrix}
\frac{\partial z_1}{\partial x_1} & \frac{\partial z_1}{\partial x_2} & \cdots & \frac{\partial z_1}{\partial x_D} \\
\frac{\partial z_2}{\partial x_1} & \frac{\partial z_2}{\partial x_2} & \cdots & \frac{\partial z_2}{\partial x_D} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial z_M}{\partial x_1} & \frac{\partial z_M}{\partial x_2} & \cdots & \frac{\partial z_M}{\partial x_D}
\end{bmatrix}
=
\begin{bmatrix}
W_{1,1} & W_{2,1} & \cdots & W_{D,1} \\
W_{1,2} & W_{2,2} & \cdots & W_{D,2} \\
\vdots  & \vdots  & \ddots & \vdots \\
W_{1,M} & W_{2,M} & \cdots & W_{D,M}
\end{bmatrix}
= W^T \in \mathbb{R}^{M \times D}
$$

## 2 np中axis、keepdims的简单理解

一个矩阵shape = (n, m)

keepdims = True 时，对他的axis=0进行计算，就是把 shape[0] 变成 1，(1, m)；对axis=1进行计算，就是把 shape[1] 变成 1，(n, 1)

keepdims = False时，直接变成一维向量，结果分别为(m,) 和 (n,)



对 a.shape=(2, 3)：

| **操作**                         | **含义**               | **输出形状** |
| -------------------------------- | ---------------------- | ------------ |
| np.sum(a, axis=0)                | 每列相加               | (3,)         |
| np.sum(a, axis=1)                | 每行相加               | (2,)         |
| np.sum(a, axis=0, keepdims=True) | 每列相加并保留“行”维度 | (1, 3)       |
| np.sum(a, axis=1, keepdims=True) | 每行相加并保留“列”维度 | (2, 1)       |

